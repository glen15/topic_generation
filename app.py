import streamlit as st
import boto3
import json
import os
from datetime import datetime
from botocore.exceptions import ClientError

# AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def get_bedrock_client():
    try:
        return boto3.client('bedrock-runtime', region_name='us-east-1')
    except Exception as e:
        st.error(f"AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

def generate_hackathon_idea_with_nova(problem_area, target_problem, ai_technology, target_users, expected_impact, idea_length="ë³´í†µ", debug_mode=False):
    """Nova Lite ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬ë¹™ë© í•´ì»¤í†¤ ì•„ì´ë””ì–´ ìƒì„±"""
    bedrock_client = get_bedrock_client()
    
    if not bedrock_client:
        return "âŒ AWS Bedrock ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    # ê¸¸ì´ ì˜µì…˜ì— ë”°ë¥¸ ì„¤ì • (í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤í•˜ì—¬ í† í° ìˆ˜ ì¦ê°€)
    length_settings = {
        "ê°„ë‹¨": {
            "char_limit": 800,
            "max_tokens": 1000,  # 800ì Ã— 1.25 (í•œêµ­ì–´ ì—¬ìœ ë¶„)
            "sections": {
                "title": "10ì ì´ë‚´",
                "overview": "80ì ì´ë‚´",
                "problem": "60ì ì´ë‚´", 
                "ai_tech": "80ì ì´ë‚´",
                "users": "40ì ì´ë‚´",
                "features": "120ì ì´ë‚´, 3ê°œ ê¸°ëŠ¥",
                "impact": "80ì ì´ë‚´",
                "tech_stack": "60ì ì´ë‚´",
                "test_plan": "60ì ì´ë‚´",
                "expansion": "60ì ì´ë‚´"
            }
        },
        "ë³´í†µ": {
            "char_limit": 1500,
            "max_tokens": 2000,  # 1500ì Ã— 1.33 (í•œêµ­ì–´ ì—¬ìœ ë¶„)
            "sections": {
                "title": "20ì ì´ë‚´",
                "overview": "150ì ì´ë‚´",
                "problem": "100ì ì´ë‚´",
                "ai_tech": "150ì ì´ë‚´",
                "users": "80ì ì´ë‚´", 
                "features": "200ì ì´ë‚´, 3-4ê°œ ê¸°ëŠ¥",
                "impact": "150ì ì´ë‚´",
                "tech_stack": "100ì ì´ë‚´",
                "test_plan": "120ì ì´ë‚´",
                "expansion": "100ì ì´ë‚´"
            }
        },
        "ìƒì„¸": {
            "char_limit": 2500,
            "max_tokens": 3200,  # 2500ì Ã— 1.28 (í•œêµ­ì–´ ì—¬ìœ ë¶„)
            "sections": {
                "title": "30ì ì´ë‚´",
                "overview": "250ì ì´ë‚´",
                "problem": "200ì ì´ë‚´",
                "ai_tech": "300ì ì´ë‚´",
                "users": "150ì ì´ë‚´",
                "features": "400ì ì´ë‚´, 4-5ê°œ ê¸°ëŠ¥",
                "impact": "250ì ì´ë‚´",
                "tech_stack": "200ì ì´ë‚´",
                "test_plan": "200ì ì´ë‚´",
                "expansion": "200ì ì´ë‚´"
            }
        }
    }
    
    settings = length_settings[idea_length]
    sections = settings["sections"]
    
    prompt = f"""
ë‹¹ì‹ ì€ ì§€ì†ê°€ëŠ¥í•œ ì„¸ìƒì„ ìœ„í•œ ë¦¬ë¹™ë© í•´ì»¤í†¤ì˜ ì „ë¬¸ ë©˜í† ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì°½ì˜ì ì´ê³  ì‹¤í˜„ ê°€ëŠ¥í•œ í•´ì»¤í†¤ ì•„ì´ë””ì–´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ì…ë ¥ ì •ë³´:
- ë¬¸ì œ ì˜ì—­: {problem_area}
- í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œ: {target_problem}
- í™œìš©í•  AI ê¸°ìˆ : {ai_technology}
- íƒ€ê²Ÿ ì‚¬ìš©ì: {target_users}
- ê¸°ëŒ€ íš¨ê³¼: {expected_impact}

**ì¤‘ìš”**: ê° ì„¹ì…˜ì€ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì „ì²´ ì‘ë‹µì€ {settings["char_limit"]}ì ì´ë‚´ë¡œ ì œí•œí•©ë‹ˆë‹¤.

ë‹¤ìŒ êµ¬ì¡°ë¡œ í•´ì»¤í†¤ ì•„ì´ë””ì–´ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”:

## ğŸ¯ í”„ë¡œì íŠ¸ ì œëª©
({sections["title"]}ì˜ ì°½ì˜ì ì´ê³  ì„íŒ©íŠ¸ ìˆëŠ” í”„ë¡œì íŠ¸ëª…)

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš” ({sections["overview"]})
í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ê³¼ ëª©ì ì„ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì„¤ëª…

## ğŸŒ í•´ê²° ë¬¸ì œ ({sections["problem"]})
êµ¬ì²´ì ì¸ ë¬¸ì œ ì •ì˜ì™€ í˜„ì¬ ìƒí™©ì„ ì„¤ëª…

## ğŸ¤– AI ê¸°ìˆ  í™œìš© ({sections["ai_tech"]})
ì–´ë–¤ AI ê¸°ìˆ ì„ ì–´ë–»ê²Œ í™œìš©í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…

## ğŸ‘¥ íƒ€ê²Ÿ ì‚¬ìš©ì ({sections["users"]})
ì£¼ìš” ì‚¬ìš©ìì™€ ì´í•´ê´€ê³„ìë¥¼ ë‚˜ì—´

## ğŸ’¡ í•µì‹¬ ê¸°ëŠ¥ ({sections["features"]})
ì£¼ìš” ê¸°ëŠ¥ì„ ê°„ë‹¨í•œ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ì—´
- ê¸°ëŠ¥ 1: (í•œ ì¤„ ì„¤ëª…)
- ê¸°ëŠ¥ 2: (í•œ ì¤„ ì„¤ëª…)
- ê¸°ëŠ¥ 3: (í•œ ì¤„ ì„¤ëª…)

## ğŸŠ ê¸°ëŒ€ íš¨ê³¼ ({sections["impact"]})
ì§€ì†ê°€ëŠ¥ì„± ì¸¡ë©´ì—ì„œì˜ ê¸°ëŒ€íš¨ê³¼ë¥¼ êµ¬ì²´ì  ìˆ˜ì¹˜ë‚˜ ê²°ê³¼ë¡œ ì„¤ëª…

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ ({sections["tech_stack"]})
ê°œë°œì— í•„ìš”í•œ í•µì‹¬ ê¸°ìˆ ë“¤ì„ ë‚˜ì—´

## ğŸ“Š ì‹¤ì¦ ê³„íš ({sections["test_plan"]})
ì‹¤ì œ í™˜ê²½ì—ì„œì˜ í…ŒìŠ¤íŠ¸ ë°©ë²•ì„ ì„¤ëª…

## ğŸš€ í™•ì¥ ê°€ëŠ¥ì„± ({sections["expansion"]})
í–¥í›„ ë°œì „ ë°©í–¥ì„ ì„¤ëª…

í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ê° ì„¹ì…˜ì€ ì§€ì •ëœ ê¸€ì ìˆ˜ë¥¼ ì—„ê²©íˆ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”. ì‹¤í˜„ ê°€ëŠ¥í•˜ë©´ì„œë„ í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.
"""

    try:
        # Nova Lite ëª¨ë¸ í˜¸ì¶œ (ì˜¬ë°”ë¥¸ í˜•ì‹)
        response = bedrock_client.invoke_model(
            modelId="amazon.nova-lite-v1:0",  # Nova Lite ëª¨ë¸ ID
            body=json.dumps({
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ],
                "inferenceConfig": {
                    "maxTokens": settings["max_tokens"],  # ê¸¸ì´ ì˜µì…˜ì— ë”°ë¼ ë™ì  ì„¤ì •
                    "temperature": 0.7,
                    "topP": 0.9
                }
            })
        )
        
        # ì‘ë‹µ íŒŒì‹±
        response_body = json.loads(response.get('body').read())
        
        # ë””ë²„ê¹…: ì „ì²´ ì‘ë‹µ êµ¬ì¡° í™•ì¸ (ë””ë²„ê¹… ëª¨ë“œì¼ ë•Œë§Œ)
        if debug_mode:
            st.write("### ğŸ” ë””ë²„ê¹… ì •ë³´")
            st.write("**ì‘ë‹µ êµ¬ì¡°:**", response_body.keys())
        
        # Nova ëª¨ë¸ì˜ ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if 'output' in response_body and 'message' in response_body['output']:
            content = response_body['output']['message']['content']
            if content and len(content) > 0 and 'text' in content[0]:
                generated_text = content[0]['text']
                
                if debug_mode:
                    # ë””ë²„ê¹… ì •ë³´
                    st.write(f"**ìƒì„±ëœ í…ìŠ¤íŠ¸ ê¸¸ì´:** {len(generated_text)}ì")
                    st.write(f"**ìš”ì²­í•œ ìµœëŒ€ í† í°:** {settings['max_tokens']}")
                    st.write(f"**ëª©í‘œ ê¸€ì ìˆ˜:** {settings['char_limit']}ì")
                    
                    # í† í° ì‚¬ìš©ëŸ‰ í™•ì¸ (ìˆëŠ” ê²½ìš°)
                    if 'usage' in response_body:
                        usage = response_body['usage']
                        st.write(f"**ì‹¤ì œ ì‚¬ìš© í† í°:** {usage}")
                
                # ì‘ë‹µì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸ (í•­ìƒ ì²´í¬í•˜ì§€ë§Œ ë©”ì‹œì§€ëŠ” ì¡°ê±´ë¶€)
                if 'stopReason' in response_body.get('output', {}):
                    stop_reason = response_body['output']['stopReason']
                    
                    if debug_mode:
                        st.write(f"**ì‘ë‹µ ì¢…ë£Œ ì´ìœ :** {stop_reason}")
                    
                    if stop_reason == 'max_tokens':
                        if debug_mode:
                            st.warning("âš ï¸ í† í° ì œí•œìœ¼ë¡œ ì¸í•´ ì‘ë‹µì´ ì˜ë ¸ìŠµë‹ˆë‹¤!")
                        
                        # ìë™ ì¬ì‹œë„ (í† í° ìˆ˜ ì¦ê°€) - ë””ë²„ê¹… ëª¨ë“œì™€ ê´€ê³„ì—†ì´ ì‹¤í–‰
                        if settings["max_tokens"] < 4000:  # ìµœëŒ€ 4000 í† í°ê¹Œì§€
                            if debug_mode:
                                st.info("ğŸ”„ ë” ê¸´ ì‘ë‹µì„ ìœ„í•´ ìë™ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                            
                            increased_tokens = min(settings["max_tokens"] + 1000, 4000)
                            
                            # ì¬ì‹œë„ ìš”ì²­
                            retry_response = bedrock_client.invoke_model(
                                modelId="amazon.nova-lite-v1:0",
                                body=json.dumps({
                                    "messages": [
                                        {
                                            "role": "user",
                                            "content": [
                                                {
                                                    "text": prompt
                                                }
                                            ]
                                        }
                                    ],
                                    "inferenceConfig": {
                                        "maxTokens": increased_tokens,
                                        "temperature": 0.7,
                                        "topP": 0.9
                                    }
                                })
                            )
                            
                            retry_body = json.loads(retry_response.get('body').read())
                            if 'output' in retry_body and 'message' in retry_body['output']:
                                retry_content = retry_body['output']['message']['content']
                                if retry_content and len(retry_content) > 0 and 'text' in retry_content[0]:
                                    retry_text = retry_content[0]['text']
                                    if debug_mode:
                                        st.write(f"**ì¬ì‹œë„ ê²°ê³¼ ê¸¸ì´:** {len(retry_text)}ì")
                                    return retry_text
                    
                    elif stop_reason == 'end_turn' and debug_mode:
                        st.success("âœ… ì‘ë‹µì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                return generated_text
        
        return 'í•´ì»¤í†¤ ì•„ì´ë””ì–´ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
        
    except ClientError as e:
        return f"âŒ AWS API í˜¸ì¶œ ì˜¤ë¥˜: {e}"
    except Exception as e:
        return f"âŒ í•´ì»¤í†¤ ì•„ì´ë””ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def generate_streamlit_prd(idea_content):
    """Nova Lite ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ Streamlit ì•± PRD ìƒì„±"""
    bedrock_client = get_bedrock_client()
    
    if not bedrock_client:
        return "âŒ AWS Bedrock ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    prompt = f"""
ë‹¤ìŒ í•´ì»¤í†¤ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì´ˆê¸° MVP(Minimum Viable Product)** ë²„ì „ì˜ Streamlit ì•± êµ¬í˜„ì„ ìœ„í•œ ê°„ë‹¨í•œ PRDë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

í•´ì»¤í†¤ ì•„ì´ë””ì–´:
{idea_content}

**ì¤‘ìš” ì œì•½ì‚¬í•­:**
- ì´ê²ƒì€ ì´ˆê¸° MVP ë²„ì „ì´ë¯€ë¡œ í•µì‹¬ ê¸°ëŠ¥ë§Œ í¬í•¨
- AI ê¸°ëŠ¥ì€ **í…ìŠ¤íŠ¸ ì²˜ë¦¬ë§Œìœ¼ë¡œ ì œí•œ** (ì´ë¯¸ì§€, ìŒì„±, ì˜ìƒ ì²˜ë¦¬ ì œì™¸)
- ë³µì¡í•œ AI ëª¨ë¸ë³´ë‹¤ëŠ” ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„ì„, ë¶„ë¥˜, ìš”ì•½ ë“±ì— ì§‘ì¤‘
- ì‹¤ì œ 1-2ì¼ ë‚´ì— êµ¬í˜„ ê°€ëŠ¥í•œ ë²”ìœ„ë¡œ í•œì •

ë‹¤ìŒ êµ¬ì¡°ë¡œ ê°„ë‹¨í•˜ê³  ì‹¤ìš©ì ì¸ PRDë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

# í”„ë¡œì íŠ¸ëª… (MVP ë²„ì „)

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”
- **ëª©ì **: (í•œ ì¤„ ì„¤ëª…)
- **íƒ€ê²Ÿ ì‚¬ìš©ì**: (ì£¼ìš” ì‚¬ìš©ì)
- **MVP ë²”ìœ„**: í…ìŠ¤íŠ¸ ê¸°ë°˜ AI ê¸°ëŠ¥ë§Œ í¬í•¨

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥ (MVP í•µì‹¬)
### í•„ìˆ˜ ê¸°ëŠ¥ (í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•œì •)
1. ê¸°ëŠ¥ 1 - í…ìŠ¤íŠ¸ ì…ë ¥ ë° ì²˜ë¦¬
2. ê¸°ëŠ¥ 2 - í…ìŠ¤íŠ¸ ë¶„ì„/ë¶„ë¥˜/ìš”ì•½ ë“±
3. ê¸°ëŠ¥ 3 - ê²°ê³¼ í‘œì‹œ ë° í”¼ë“œë°±

### ì œì™¸ ê¸°ëŠ¥ (í–¥í›„ ë²„ì „)
- ì´ë¯¸ì§€/ìŒì„±/ì˜ìƒ ì²˜ë¦¬
- ë³µì¡í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

## ğŸ“± Streamlit ì•± êµ¬ì„±
### í™”ë©´ êµ¬ì„±
- **ë©”ì¸ í˜ì´ì§€**: í…ìŠ¤íŠ¸ ì…ë ¥ ë° ì„¤ì •
- **ê²°ê³¼ í˜ì´ì§€**: ì²˜ë¦¬ ê²°ê³¼ ë° ë¶„ì„

### ì‚¬ìš©í•  Streamlit ì»´í¬ë„ŒíŠ¸
- ì…ë ¥: `st.text_input()`, `st.text_area()`, `st.selectbox()`
- ì¶œë ¥: `st.write()`, `st.markdown()`, `st.dataframe()`
- ìƒí˜¸ì‘ìš©: `st.button()`, `st.tabs()`, `st.expander()`

## ğŸ’¾ ë°ì´í„° ì²˜ë¦¬ (í…ìŠ¤íŠ¸ë§Œ)
- **ì…ë ¥ ë°ì´í„°**: ì‚¬ìš©ì í…ìŠ¤íŠ¸ ì…ë ¥
- **ì²˜ë¦¬ ê³¼ì •**: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„ì„/ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜
- **ì¶œë ¥ í˜•íƒœ**: í…ìŠ¤íŠ¸ ê²°ê³¼, ì°¨íŠ¸, í‘œ í˜•íƒœ

## ğŸ¤– AI ê¸°ëŠ¥ (í…ìŠ¤íŠ¸ í•œì •)
- **ì‚¬ìš© ëª¨ë¸**: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë˜ëŠ” API
- **ì²˜ë¦¬ ë²”ìœ„**: í…ìŠ¤íŠ¸ ë¶„ë¥˜, í‚¤ì›Œë“œ ì¶”ì¶œ, ê°ì • ë¶„ì„, ìš”ì•½ ë“±
- **ì œì™¸ í•­ëª©**: ì´ë¯¸ì§€/ìŒì„±/ì˜ìƒ AI ê¸°ëŠ¥

## ğŸ“š í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
streamlit
pandas
matplotlib (ë˜ëŠ” plotly)
requests (API ì‚¬ìš©ì‹œ)
nltk ë˜ëŠ” spacy (í…ìŠ¤íŠ¸ ì²˜ë¦¬)
openai ë˜ëŠ” transformers (í…ìŠ¤íŠ¸ AI, ì„ íƒì )
```

## âš¡ êµ¬í˜„ ìˆœì„œ (MVP ê¸°ì¤€)
1. **1ë‹¨ê³„**: ê¸°ë³¸ UI ë° í…ìŠ¤íŠ¸ ì…ë ¥ êµ¬ì„±
2. **2ë‹¨ê³„**: í•µì‹¬ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê¸°ëŠ¥ êµ¬í˜„
3. **3ë‹¨ê³„**: ê²°ê³¼ í‘œì‹œ ë° ê¸°ë³¸ ê°œì„ 

## ğŸš€ í–¥í›„ í™•ì¥ ê³„íš
- 2ì°¨ ë²„ì „: ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€
- 3ì°¨ ë²„ì „: ë” ë³µì¡í•œ AI ëª¨ë¸ ì ìš©

**MVP ë²„ì „ìœ¼ë¡œ ê°„ë‹¨í•˜ê³  ì‹¤ìš©ì ìœ¼ë¡œ ì‘ì„±í•˜ë˜, í…ìŠ¤íŠ¸ ê¸°ë°˜ AI ê¸°ëŠ¥ë§Œ í¬í•¨í•˜ì—¬ ì‹¤ì œ êµ¬í˜„ ê°€ëŠ¥í•œ ë‚´ìš©ìœ¼ë¡œ í•´ì£¼ì„¸ìš”.**
"""

    try:
        # Nova Lite ëª¨ë¸ í˜¸ì¶œ
        response = bedrock_client.invoke_model(
            modelId="amazon.nova-lite-v1:0",
            body=json.dumps({
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ],
                "inferenceConfig": {
                    "maxTokens": 1500,  # ê°„ë‹¨í•œ PRDìš©ìœ¼ë¡œ í† í° ìˆ˜ ì¤„ì„
                    "temperature": 0.7,
                    "topP": 0.9
                }
            })
        )
        
        # ì‘ë‹µ íŒŒì‹±
        response_body = json.loads(response.get('body').read())
        
        # Nova ëª¨ë¸ì˜ ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if 'output' in response_body and 'message' in response_body['output']:
            content = response_body['output']['message']['content']
            if content and len(content) > 0 and 'text' in content[0]:
                return content[0]['text']
        
        return 'PRD ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
        
    except ClientError as e:
        return f"âŒ AWS API í˜¸ì¶œ ì˜¤ë¥˜: {e}"
    except Exception as e:
        return f"âŒ PRD ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def save_prd_to_markdown(prd_content, filename=None):
    """PRD ë‚´ìš©ì„ Markdown íŒŒì¼ë¡œ ì €ì¥"""
    try:
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"streamlit_app_prd_{timestamp}.md"
        
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥
        filepath = os.path.join(os.getcwd(), filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(prd_content)
        
        return True, filepath
    except Exception as e:
        return False, str(e)

# ì•± ì œëª©
st.title("ğŸŒ± AI Ã— ì§€ì†ê°€ëŠ¥ì„± ë¦¬ë¹™ë© í•´ì»¤í†¤ ì•„ì´ë””ì–´ ìƒì„±ê¸°")

# íƒ­ ìƒì„±
tab1, tab2 = st.tabs(["ğŸ’¡ ì•„ì´ë””ì–´ ìƒì„±", "ğŸ“‹ PRD ìƒì„±"])

with tab1:
    # ì…ë ¥ í•„ë“œë“¤
    st.write("## ğŸ’¡ ì•„ì´ë””ì–´ í•µì‹¬ ìš”ì†Œ ì…ë ¥")

    problem_area = st.selectbox(
        "ğŸŒ ë¬¸ì œ ì˜ì—­",
        ["í™˜ê²½ ë³´í˜¸", "ì—ë„ˆì§€ íš¨ìœ¨", "íê¸°ë¬¼ ê´€ë¦¬", "ì§€ì†ê°€ëŠ¥í•œ ë†ì—…", "ìŠ¤ë§ˆíŠ¸ ì‹œí‹°", "ê¸°í›„ ë³€í™” ëŒ€ì‘", "ìˆœí™˜ ê²½ì œ", "ì¹œí™˜ê²½ êµí†µ", "ìˆ˜ìì› ê´€ë¦¬", "ìƒë¬¼ ë‹¤ì–‘ì„± ë³´ì „", "ê¸°íƒ€"]
    )

    target_problem = st.text_area(
        "ğŸ¯ í•´ê²°í•˜ê³ ì í•˜ëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œ",
        "ì˜ˆ: ìŒì‹ë¬¼ ì“°ë ˆê¸° ì¦ê°€ë¡œ ì¸í•œ í™˜ê²½ ì˜¤ì—¼ê³¼ ìì› ë‚­ë¹„ ë¬¸ì œ"
    )

    ai_technology = st.text_input(
        "ğŸ¤– í™œìš©í•  AI ê¸°ìˆ ",
        "ì˜ˆ: ì»´í“¨í„° ë¹„ì „, ìì—°ì–´ ì²˜ë¦¬, ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ëª¨ë¸"
    )

    target_users = st.text_input(
        "ğŸ‘¥ íƒ€ê²Ÿ ì‚¬ìš©ì",
        "ì˜ˆ: ì¼ë°˜ ê°€ì •, ì‹ë‹¹ ìš´ì˜ì, ì§€ìì²´"
    )

    expected_impact = st.text_area(
    "ğŸŠ ê¸°ëŒ€í•˜ëŠ” ì§€ì†ê°€ëŠ¥ì„± íš¨ê³¼",
    "ì˜ˆ: ìŒì‹ë¬¼ ì“°ë ˆê¸° 30% ê°ì†Œ, CO2 ë°°ì¶œëŸ‰ ì €ê°, ìì› ìˆœí™˜ ì´‰ì§„"
    )

    st.write("---")

    # ì˜µì…˜ ì„¤ì •
    col1, col2 = st.columns([3, 1])

    with col1:
        # ì•„ì´ë””ì–´ ê¸¸ì´ ì„ íƒ
        idea_length = st.selectbox(
        "ğŸ“ ì•„ì´ë””ì–´ ìƒì„± ë²”ìœ„",
        ["ê°„ë‹¨", "ë³´í†µ", "ìƒì„¸"],
        index=1,  # ê¸°ë³¸ê°’: ë³´í†µ
    )

    with col2:
        # ë””ë²„ê¹… ëª¨ë“œ ì²´í¬ë°•ìŠ¤
        debug_mode = st.checkbox("ğŸ” ë””ë²„ê¹… ëª¨ë“œ", help="AI ì‘ë‹µ ë¶„ì„ ì •ë³´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤")

        # ê¸¸ì´ ì •ë³´ í‘œì‹œ
        length_info = {
            "ê°„ë‹¨": {"chars": "800ì", "time": "âš¡ ë¹ ë¦„", "desc": "í•µì‹¬ë§Œ ê°„ëµíˆ"},
            "ë³´í†µ": {"chars": "1,500ì", "time": "âš–ï¸ ê· í˜•", "desc": "ì ë‹¹í•œ ìƒì„¸ë„"}, 
            "ìƒì„¸": {"chars": "2,500ì", "time": "ğŸ” ìƒì„¸", "desc": "ì¶©ë¶„í•œ ì„¤ëª…"}
        }
        
        info = length_info[idea_length]
        st.write(f"**{info['time']}**")
        st.write(f"ğŸ“ {info['chars']}")
        st.write(f"ğŸ’¡ {info['desc']}")

    # ìƒì„± ë²„íŠ¼
    if st.button("ğŸš€ í•´ì»¤í†¤ ì•„ì´ë””ì–´ ìƒì„±í•˜ê¸°", type="primary"):
        if problem_area and target_problem and ai_technology and target_users and expected_impact:
            with st.spinner("AIê°€ í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                generated_idea = generate_hackathon_idea_with_nova(
                    problem_area, target_problem, ai_technology, target_users, expected_impact, idea_length, debug_mode
                )
            
            # ìƒì„±ëœ ì•„ì´ë””ì–´ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state.current_idea = {
                "generated_content": generated_idea,
                "idea_length": idea_length
            }
            st.session_state.idea_generated = True
            
            st.success("âœ… ì•„ì´ë””ì–´ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            st.error("ëª¨ë“  í•„ë“œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”!")

    # ìƒì„±ëœ ì•„ì´ë””ì–´ í‘œì‹œ ë° ì €ì¥ ê¸°ëŠ¥ (ì„¸ì…˜ ìƒíƒœ ê¸°ë°˜)
    if hasattr(st.session_state, 'idea_generated') and st.session_state.idea_generated and hasattr(st.session_state, 'current_idea'):
        current_idea = st.session_state.current_idea
        
        # ì•„ì´ë””ì–´ í—¤ë” (ê¸¸ì´ ì •ë³´ í¬í•¨)
        col1, col2 = st.columns([3, 1])
        with col1:
            st.write("## ğŸ“‹ ìƒì„±ëœ í•´ì»¤í†¤ ì•„ì´ë””ì–´")
        with col2:
            idea_length_used = current_idea.get('idea_length', 'ë³´í†µ')
            length_badge = {
                "ê°„ë‹¨": "âš¡ ê°„ë‹¨í˜• (800ì)",
                "ë³´í†µ": "âš–ï¸ í‘œì¤€í˜• (1,500ì)",
                "ìƒì„¸": "ğŸ” ìƒì„¸í˜• (2,500ì)"
            }
            st.info(f"ğŸ“ {length_badge.get(idea_length_used, 'í‘œì¤€í˜•')}")
        
            # ì‹¤ì œ ê¸€ì ìˆ˜ ê³„ì‚° ë° í‘œì‹œ
        content_length = len(current_idea['generated_content'])
        st.caption(f"ğŸ“Š ì‹¤ì œ ìƒì„±ëœ ê¸€ì ìˆ˜: {content_length:,}ì")
        
        st.markdown(current_idea['generated_content'])
        
        st.write("---")
        
        # ìƒˆ ì•„ì´ë””ì–´ ìƒì„± ë²„íŠ¼
        if st.button("ğŸ”„ ìƒˆ ì•„ì´ë””ì–´ ìƒì„±", key="new_idea_button", type="primary"):
            # í˜„ì¬ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            if 'current_idea' in st.session_state:
                del st.session_state.current_idea
            if 'idea_generated' in st.session_state:
                del st.session_state.idea_generated
            st.rerun()

    st.write("---")
    st.info("ğŸŒ± ì§€ì†ê°€ëŠ¥í•œ ì„¸ìƒì„ ìœ„í•œ í˜ì‹ ì ì¸ AI ì†”ë£¨ì…˜ ì•„ì´ë””ì–´ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”! AWS Bedrock Nova Liteê°€ ë„ì™€ë“œë¦½ë‹ˆë‹¤.")

    with tab2:
        st.write("## ğŸ“‹ ê°„ë‹¨í•œ Streamlit ì•± PRD ìƒì„±")
        
        # ì•„ì´ë””ì–´ ì…ë ¥ ë°©ì‹ ì„ íƒ
        input_method = st.radio(
            "ğŸ“¥ ì•„ì´ë””ì–´ ì…ë ¥ ë°©ì‹",
            ["ì§ì ‘ ì…ë ¥", "ì•„ì´ë””ì–´ ìƒì„± íƒ­ì—ì„œ ê°€ì ¸ì˜¤ê¸°"],
            horizontal=True
        )
        
        if input_method == "ì•„ì´ë””ì–´ ìƒì„± íƒ­ì—ì„œ ê°€ì ¸ì˜¤ê¸°":
            if hasattr(st.session_state, 'current_idea') and 'generated_content' in st.session_state.current_idea:
                st.write("### ğŸ“ ê°€ì ¸ì˜¨ ì•„ì´ë””ì–´")
                with st.expander("ìƒì„±ëœ ì•„ì´ë””ì–´ ë‚´ìš© ë³´ê¸°"):
                    st.markdown(st.session_state.current_idea['generated_content'])
                idea_content = st.session_state.current_idea['generated_content']
            else:
                st.warning("âš ï¸ ì•„ì´ë””ì–´ ìƒì„± íƒ­ì—ì„œ ë¨¼ì € ì•„ì´ë””ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
                idea_content = ""
        else:
            idea_content = st.text_area(
                "ğŸ’¡ í•´ì»¤í†¤ ì•„ì´ë””ì–´ ë‚´ìš©",
                height=200,
                placeholder="ìƒì„±ëœ í•´ì»¤í†¤ ì•„ì´ë””ì–´ë¥¼ ì—¬ê¸°ì— ë¶™ì—¬ë„£ì–´ ì£¼ì„¸ìš”..."
            )
        
        st.write("---")
        
        # PRD ìƒì„± ë²„íŠ¼
        if st.button("ğŸ“‹ ê°„ë‹¨í•œ PRD ìƒì„±í•˜ê¸°", type="primary"):
            if idea_content.strip():
                with st.spinner("ê°„ë‹¨í•œ Streamlit ì•± PRDë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    prd_content = generate_streamlit_prd(idea_content)
                
                st.write("## ğŸ“‹ ìƒì„±ëœ PRD")
                st.markdown(prd_content)
                
                # PRDë¥¼ ì„¸ì…˜ì— ì €ì¥
                st.session_state.current_prd = prd_content
                
                st.write("---")
                
                # íŒŒì¼ ì €ì¥ ì˜µì…˜
                col1, col2 = st.columns([1, 3])
                with col1:
                    if st.button("ğŸ’¾ MD íŒŒì¼ë¡œ ì €ì¥", type="secondary"):
                        success, result = save_prd_to_markdown(prd_content)
                        if success:
                            st.success(f"âœ… PRDê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            st.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: `{result}`")
                        else:
                            st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {result}")
                
                with col2:
                    st.download_button(
                        label="ğŸ“¥ MD íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        data=prd_content,
                        file_name=f"streamlit_app_prd_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                        mime="text/markdown"
                    )
            else:
                st.error("âŒ ì•„ì´ë””ì–´ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        
        # ê¸°ì¡´ PRD í‘œì‹œ (ì„¸ì…˜ì— ìˆëŠ” ê²½ìš°)
        if hasattr(st.session_state, 'current_prd'):
            st.write("---")
            st.write("### ğŸ“‹ í˜„ì¬ ì„¸ì…˜ì˜ PRD")
            with st.expander("PRD ë‚´ìš© ë³´ê¸°"):
                st.markdown(st.session_state.current_prd)
                
                # ê¸°ì¡´ PRDë„ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥
                st.download_button(
                    label="ğŸ“¥ í˜„ì¬ PRD ë‹¤ìš´ë¡œë“œ",
                    data=st.session_state.current_prd,
                    file_name=f"current_prd_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                    mime="text/markdown",
                    key="download_current_prd"
                ) 
